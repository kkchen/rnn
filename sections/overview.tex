\section{Overview}
\subsection{}

\begin{frame}{Resources}
    \begin{itemize}
        \item Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
        \emph{Deep Learning}, ch.~10. MIT Press, 2016
        \nocite{GoodfellowDL}
        \begin{itemize}
            \item Excellent primer
            \item \blueurl{http://www.deeplearningbook.org}
        \end{itemize}
        \item These slides: \blueurl{https://github.com/kkchen/rnn}
        \item References at the end of these slides
        \item Talk to me! \smiley
    \end{itemize}
\end{frame}

\begin{frame}{Why might I want an RNN?}
    \begin{itemize}
        \item<+-> Universal approximation theorem: dense NNs can model anything nice
        \begin{itemize}
            \item But doesn't specify how hard it is to train
        \end{itemize}
        \item<+-> Silly example: 5 seconds of 44,100 samples/s speech audio $\rightarrow$ 14 transcribed words
        \begin{itemize}
            \item $\exists$ $\sim$3000 commonly-used English words\footnote{The Internet}
            \item $\Reals^{220,500} \to \Reals^{42,000}$ deep dense network?
            \begin{itemize}
                \item Theoretically possible, but please don't
            \end{itemize}
            \item What if I speak 15 words over 5.01~s?
            Must start over!
        \end{itemize}
        \item<+-> More relevant example: predict some quantity from a fluid flow
        \begin{itemize}
            \item Dynamics (e.g., Navier--Stokes) do not change from time $t$ to $t + \Delta t$
            \item Dense network does not know this; allows any dynamical behavior at any $t$
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{What RNNs do}
    \begin{block}{Key motivation}<+->
        Many processes are sequences through \alert{time} or \alert{steps}, with \alert{time-invariant dynamics}
    \end{block}

    \begin{block}{RNNs are input--output models that impose this structure}<+->
        Interpretation for mechanical/aerospace/electrical engineers: RNNs are \alert{discrete-time nonlinear time-invariant dynamical systems} in which the input/state/output dynamics are learned from (i.e., fitted to) data
    \end{block}

    \begin{itemize}
        \item<+-> Note: RNN states usually very difficult to interpret
        \item<.-> RNNs assign same dynamics @ time step 213,913 as 213,914
        \begin{itemize}
            \item<.-> Dense NNs treat them entirely separately
        \end{itemize}
        \item<+-> Easier to converge to model with ``reasonable''/``desirable'' behavior
        \item<.-> Far fewer parameters than naive dense networks
        \item<.-> Flexible in how many steps to take
    \end{itemize}
\end{frame}

\begin{frame}{Unfolded computational graphs \& circuit diagrams}
    \input{figures/rnn}
\end{frame}

\begin{frame}{Recurrent neural networks in one slide}
    \begin{itemize}[<+->]
        \item Express in terms of discrete-time dynamical systems: $t = 0, 1, \dots, \tau$
        \item $\s_t \in \Reals^n$: (cell) state; initial condition $\s_0$ (often $\zero$)
        \item<.-> $\x_t \in \Reals^q$: inputs
        \item $\f: \Reals^n \times \Reals^q \to \Reals^n$: state update
        \item<.-> $\THETA \in \Reals^m$: parameters for $\f$
        \item State update:
        \begin{equation*}
            \s_t = \f(\s_{t-1}, \x_t; \THETA),
            \quad \text{e.g.}, \,
            \s_t = \tanh(\W \s_{t-1} + \U \x_t + \b), \;
            \THETA = (\W, \U, \b)
        \end{equation*}
        \item Outputs: $\y_t \in \Reals^p$, $\h: \Reals^n \to \Reals^p$, parameters $\PHI \in \Reals^l$:
        \begin{equation*}
            \y_t = \h(\s_t; \PHI),
            \quad \text{e.g.}, \,
            \y_t = \mathrm{softmax}(\V \s_{t} + \c), \;
            \PHI = (\V, \c)
        \end{equation*}
        \item That's it!  Now pick $\THETA$ and $\PHI$ to minimize
        \begin{equation*}
            \mathrm{loss} = L(
                \underbrace{(\ETA_1, \dots, \ETA_\tau)}_\text{data},
                \underbrace{(\y_1, \dots, \y_\tau)}_\text{prediction}
            )
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Backpropagation through time \& unrolling}
    \begin{itemize}
        \item<+-> I won't bore you with backprop equations; see \href{http://www.deeplearningbook.org}{\textcolor{blue}{Goodfellow's book}}
        \item<.-> It's just a normal gradient via chain rule; any optimizer can be used
        \item<+-> Need to forward and backprop through input--state--output relations, \emph{and} through time
        \begin{itemize}
            \item Forward and backprop generally serial through time
        \end{itemize}
        \item<.-> For state $\in \Reals^n$, inputs $\in \Reals^q$, outputs $\in \Reals^p$, length $\tau$:
        \begin{itemize}
            \item Backprop cost = $\O(\tau n^2) + \O(\tau n p) + \O(\tau n q)$ products
            \item Watch out for very large \# time steps and input/state/output sizes
        \end{itemize}
        \item<+-> Software typically allows you to \alert{unroll} an RNN
        \begin{itemize}
            \item Precomputes entire unfolded computational graph \& all gradients through all time steps
            \item Faster, but more memory consumption
            \item Good for small, fixed-length RNNs
        \end{itemize}
    \end{itemize}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../rnn"
%%% End:
