\section{Alternate architectures}
\subsection{}

% To do: explain HMM.
\begin{frame}{RNNs and Hidden Markov Models}
    \begin{columns}
        \begin{column}{0.46\textwidth}
            \input{figures/hmm}
        \end{column}
        \begin{column}{0.54\textwidth}
            \begin{itemize}
                \item<+-> RNNs are deterministic, HMMs are stochastic
                \item<.-> But posterior probabilities of states and outputs of HMMs are deterministic
            \end{itemize}
            \begin{block}{}<+->
                An HMM, when viewed in terms of probabilities, is a special case of an RNN
            \end{block}
            \begin{itemize}[<.->]
                \item HMM transition probabilities analogous to RNN weights
                \item<+-> Why do traditional RNNs and HMMs excel at different tasks?
                \item What about a task makes each work well?
                \item What can each do well that the other cannot?
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{1-D convolutional neural networks}
    Sequences analogous to 1-D space
    \begin{itemize}
        \item<+-> Physics/engineering analogy: time analogous to 1-D space
        \item ODE methods translate between the two
    \end{itemize}
    \uncover<+->{Can use 1-D CNNs to process sequences}

    \vspace{-4mm}
    \begin{center}
        \input{figures/cnn}
        \vspace{4mm}

        \uncover<+->{
            \begin{tabular}{c|c}
                RNN & CNN \\
                \hline
                $\y_t$ depends on $\s_0, \dots, \s_t$ &
                $\y_t$ depends on $\s_{t-\delta}, \dots, \s_{t+\delta}$ \\
                (also $\s_{t+1}, \dots, \s_\tau$ if desired) \\
                generally non-parallelizable across $t$ & parallelizable across $t$ \\
            \end{tabular}
        }
    \end{center}

    \uncover<.->{
        RNNs generally stronger than 1-D CNNs for sequences;
        RNN cell state intuitively helpful
    }
\end{frame}

\begin{frame}{Recursive neural networks}
    \input{figures/recursive.tex}

    \begin{textblock}{5.1}(6.7, 1.3)
        \begin{itemize}
            \item Introduced by \citet{PollackAI90}
            \item Advantage: longest connection $\approx \log_2 \tau$, not $\tau$
            \item Tree structure can be static or data-dependent
        \end{itemize}
    \end{textblock}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../rnn"
%%% End:
