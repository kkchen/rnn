\section{Alternate architectures}
\subsection{}

\begin{frame}{\rnn{}s and Hidden Markov Models}
    \begin{columns}
        \begin{column}{0.46\textwidth}
            \input{figures/hmm}
        \end{column}
        \begin{column}{0.54\textwidth}
            \begin{itemize}
                \item<+-> \rnn{}s are deterministic, \hmm{}s are stochastic
                \item<.-> But posterior probabilities of states and outputs of \hmm{}s are deterministic
            \end{itemize}
            \begin{block}{}<+->
                An \hmm{}, when viewed in terms of probabilities, is a special case of an \rnn{}
            \end{block}
            \begin{itemize}[<.->]
                \item \hmm{} transition probabilities analogous to \rnn{} weights
                \item<+-> Why do traditional \rnn{}s and \hmm{}s excel at different tasks?
                \item What about a task makes each work well?
                \item What can each do well that the other cannot?
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{1-D convolutional neural networks}
    Sequences analogous to 1-D space
    \begin{itemize}
        \item<+-> Physics/engineering analogy: time analogous to 1-D space
        \item Ordinary differential equation methods translate between the two
    \end{itemize}
    \uncover<+->{Can use 1-D \cnn{}s to process sequences}

    \begin{center}
        \input{figures/cnn}

        \uncover<+->{
            \begin{tabular}{c|c}
                \rnn & \cnn \\
                \hline
                $\y_t$ depends on $\s_0, \dots, \s_t$, also $\s_{t+1}, \dots, \s_\tau$ if desired &
                $\y_t$ depends on $\s_{t-\delta}, \dots, \s_{t+\delta}$ \\
                generally non-parallelizable across $t$ & parallelizable across $t$ \\
            \end{tabular}
        }
    \end{center}

    \uncover<.->{
        \rnn{}s generally stronger than 1-D \cnn{}s for sequences;
        \rnn{} cell state intuitively helpful
    }
\end{frame}

\begin{frame}{Recursive neural networks}
    \begin{columns}
        \begin{column}{0.61\textwidth}
            \input{figures/recursive.tex}
        \end{column}
        \begin{column}{0.39\textwidth}
            \begin{itemize}
                \item Introduced by \citet{PollackAI90}
                \item Advantage: longest connection $\approx \log_2 \tau$, rather than $\tau$
                \item Tree structure can be static or data-dependent
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../rnn"
%%% End:
